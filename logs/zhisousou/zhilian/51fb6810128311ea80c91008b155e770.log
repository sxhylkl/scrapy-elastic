2019-11-29 16:36:23 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: zhisousou)
2019-11-29 16:36:23 [scrapy.utils.log] INFO: Versions: lxml 4.3.0.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 18.4.0, Python 3.6.8 |Anaconda, Inc.| (default, Feb 21 2019, 18:30:04) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.6.1, Platform Windows-8.1-6.3.9600-SP0
2019-11-29 16:36:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'zhisousou', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 1, 'LOG_FILE': 'logs\\zhisousou\\zhilian\\51fb6810128311ea80c91008b155e770.log', 'NEWSPIDER_MODULE': 'zhisousou.spiders', 'SPIDER_MODULES': ['zhisousou.spiders']}
2019-11-29 16:36:23 [scrapy.extensions.telnet] INFO: Telnet Password: 8dd9948f3f3966a8
2019-11-29 16:36:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-11-29 16:36:27 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "firefox", "acceptInsecureCerts": true}}, "desiredCapabilities": {"browserName": "firefox", "acceptInsecureCerts": true, "marionette": true}}
2019-11-29 16:36:27 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): 127.0.0.1:54302
2019-11-29 16:36:33 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session HTTP/1.1" 200 735
2019-11-29 16:36:33 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:33 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'zhisousou.middlewares.ZhisousouDownloaderMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-11-29 16:36:33 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-11-29 16:36:33 [scrapy.middleware] INFO: Enabled item pipelines:
['zhisousou.pipelines.ZhisousouPipeline']
2019-11-29 16:36:33 [scrapy.core.engine] INFO: Spider opened
2019-11-29 16:36:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-11-29 16:36:33 [zhilian] INFO: Spider opened: zhilian
2019-11-29 16:36:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-11-29 16:36:33 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "http://zhaopin.com/"}
2019-11-29 16:36:41 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:36:41 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:43 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:36:43 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 173306
2019-11-29 16:36:43 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:43 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:36:43 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 36
2019-11-29 16:36:43 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://zhaopin.com/> (referer: None)
2019-11-29 16:36:43 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://sou.zhaopin.com/?jl=635&kw=Python&kt=3&sf=0&st=0"}
2019-11-29 16:36:45 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:36:45 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:47 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:36:47 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 936470
2019-11-29 16:36:47 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:47 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:36:47 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 68
2019-11-29 16:36:47 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sou.zhaopin.com/?jl=635&kw=Python&kt=3&sf=0&st=0> (referer: https://www.zhaopin.com/)
2019-11-29 16:36:48 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC508620126J00456780105.htm"}
2019-11-29 16:36:48 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:36:48 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:50 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:36:50 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 101601
2019-11-29 16:36:50 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:50 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:36:50 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:36:50 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC508620126J00456780105.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:36:50 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC340780835J00269329404.htm"}
2019-11-29 16:36:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:36:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:54 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:36:54 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 247670
2019-11-29 16:36:54 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:54 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:36:54 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:36:54 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC340780835J00269329404.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:36:54 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC383859710J00250641904.htm"}
2019-11-29 16:36:56 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:36:56 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:58 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:36:58 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 244348
2019-11-29 16:36:58 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:58 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:36:58 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:36:58 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:36:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC383859710J00250641904.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:36:58 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC628583138J00216661602.htm"}
2019-11-29 16:37:01 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:01 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:03 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:03 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 261041
2019-11-29 16:37:03 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:03 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:03 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:03 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC628583138J00216661602.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:03 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC201489335J00191776006.htm"}
2019-11-29 16:37:05 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:05 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:07 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:07 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 249526
2019-11-29 16:37:07 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:07 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:07 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:07 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC201489335J00191776006.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:07 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC632690830J00186373512.htm"}
2019-11-29 16:37:08 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:08 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:10 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:10 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 244078
2019-11-29 16:37:10 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:10 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:10 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:10 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC632690830J00186373512.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:10 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC000542945J00453438105.htm"}
2019-11-29 16:37:11 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:11 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:13 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:13 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 244403
2019-11-29 16:37:13 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:13 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:13 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:13 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC000542945J00453438105.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:13 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/305934717250402.htm"}
2019-11-29 16:37:14 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:14 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:16 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:16 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 251564
2019-11-29 16:37:16 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:16 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:16 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 56
2019-11-29 16:37:16 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/305934717250402.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:16 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120921762J00461062301.htm"}
2019-11-29 16:37:17 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:17 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:19 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:19 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 241564
2019-11-29 16:37:19 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:19 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:19 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:19 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC120921762J00461062301.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:19 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC632690830J00186375712.htm"}
2019-11-29 16:37:20 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:20 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:22 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:22 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 251556
2019-11-29 16:37:22 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:22 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:22 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:22 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC632690830J00186375712.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:22 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC632690830J00177962512.htm"}
2019-11-29 16:37:22 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:22 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:24 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:24 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 244859
2019-11-29 16:37:24 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:24 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:24 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:24 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC632690830J00177962512.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:24 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC632690830J00186377112.htm"}
2019-11-29 16:37:26 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:26 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:28 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:28 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 250321
2019-11-29 16:37:28 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:28 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:28 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:28 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC632690830J00186377112.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:28 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC842910190J00180735304.htm"}
2019-11-29 16:37:28 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:28 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:30 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:31 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 256573
2019-11-29 16:37:31 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:31 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:31 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:31 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC842910190J00180735304.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:31 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC842910190J00176419004.htm"}
2019-11-29 16:37:33 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:33 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:35 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:35 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 269532
2019-11-29 16:37:35 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:35 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:35 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:35 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC842910190J00176419004.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:35 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC872325800J00367286705.htm"}
2019-11-29 16:37:36 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:36 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:38 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:38 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 200 243152
2019-11-29 16:37:38 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:38 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {}
2019-11-29 16:37:38 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 64
2019-11-29 16:37:38 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://jobs.zhaopin.com/CC872325800J00367286705.htm> (referer: https://sou.zhaopin.com/?jl=635&sf=0&st=0&kw=Python&kt=3)
2019-11-29 16:37:38 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC230292410J00143370213.htm"}
2019-11-29 16:37:39 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 200 14
2019-11-29 16:37:39 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:41 [selenium.webdriver.remote.remote_connection] DEBUG: GET http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/source {}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "GET /session/fdb7d37a-28dd-4563-908c-4347aaedf287/source HTTP/1.1" 500 105
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC508620126J00431691105.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC831497740J00382125407.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC160261813J00344398806.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC268447232J00310766904.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120107107J00175069414.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC420498919J00318289704.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC142331073J00224111604.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC613516730J00140848702.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/399612635356627.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC508620126J00466404205.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC447954822J00442165203.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC134189873J00343208004.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC508620126J00433197105.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC367075483J00354346805.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC447954822J00409370703.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/457408237250035.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC824472920J00332944005.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC297085182J00440931007.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC000023354J00190524013.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC623461580J00407689207.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CCL1203372320J00362190302.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC599476231J00380747408.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC599476231J00380747308.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CCL1213923340J00207119009.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC896620630J00375792208.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC872220650J00199877409.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC862648410J00371601502.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC864094980J00435125803.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CCL1218825980J00459728307.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC470601732J00357901506.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC692695929J00472420401.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC645051428J00476628005.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC260972586J00326721903.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CZ829190630J00134964912.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC813110400J00461066101.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC645051428J00386044905.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC831062620J00390293707.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC143712520J00436820305.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC631016382J00443746507.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC569576922J00429457005.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC508620126J00335679405.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120107107J00183057614.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120107107J00197429814.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC678545520J00190671112.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC142715590J00367419908.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120107107J00197429714.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC545530129J00246780105.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120107107J00183057514.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC678545520J00160239312.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120107107J00076770814.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC196911813J00137056915.htm"}
2019-11-29 16:37:52 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:37:52 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:37:54 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 17 pages/min), scraped 0 items (at 0 items/min)
2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC142331073J00224111604.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC420498919J00318289704.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC120107107J00175069414.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC230292410J00143370213.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 96, in process_request
    source = self.driver.page_source
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 679, in page_source
    return self.execute(Command.GET_PAGE_SOURCE)['value']
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: Failed to decode response from marionette

2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC160261813J00344398806.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC831497740J00382125407.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC508620126J00431691105.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:37:54 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC268447232J00310766904.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:37:54 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:37:54 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:9200
2019-11-29 16:37:56 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4DD4E6D8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:37:56 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:37:56 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:37:56 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (2): localhost:9200
2019-11-29 16:37:58 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4DD4E748>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:37:58 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:37:58 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:37:58 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (3): localhost:9200
2019-11-29 16:38:00 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4DD4E6A0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:00 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:38:00 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:00 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (4): localhost:9200
2019-11-29 16:38:02 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4DD4E780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:02 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['中软国际科技服务有限公司'],
 'company_url': ['http://company.zhaopin.com/CZ508620120.htm'],
 'degree_need': ['本科'],
 'job_advantage': [''],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><div> '
              '1.参与团队内部相关产品的设计、研发和优化工作；</div><div> 2. '
              '持续改进和优化，支持内部软件及平台快速发展的业务；</div><div> 3. '
              '负责优化服务端性能、策略，发现瓶颈并改进性能及扩展性</div><div> 职位要求：</div><div> '
              '一年及以上python开发经验，有Java开发经验优先</div></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['14:53'],
 'salary': ['面议'],
 'title': ['Python开发工程师'],
 'work_years': ['1-3年']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4DD4E780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4DD4E780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4DD4E780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC508620126J00466404205.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC447954822J00442165203.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC508620126J00433197105.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/399612635356627.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC613516730J00140848702.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC134189873J00343208004.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC191215511J00310135408.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC826330310J00327011508.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC120914506J00446169003.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC640402330J00378495702.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC528943025J00441993307.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC542445930J00376959002.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CCL1222977140J00450113003.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://xiaoyuan.zhaopin.com/job/CC000117819J90001986000?from=sz"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CZ594912420J00305115703.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CZ185718010J00196024502.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC465341913J00108419313.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC645926286J00260826405.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC833252140J00274344707.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC833252140J00289577007.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CZ697762580J00254850503.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC250519519J00365838608.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC219838732J00353649002.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC219838732J00353648902.htm"}
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:38:02 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC824472920J00332944005.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC447954822J00409370703.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/457408237250035.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC367075483J00354346805.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC297085182J00440931007.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:02 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:02 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (5): localhost:9200
2019-11-29 16:38:04 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4DD7D5F8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:04 [elasticsearch] DEBUG: > ["python工程师"]
2019-11-29 16:38:04 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:04 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (6): localhost:9200
2019-11-29 16:38:06 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4C18>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:06 [elasticsearch] DEBUG: > ["python工程师"]
2019-11-29 16:38:06 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:06 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (7): localhost:9200
2019-11-29 16:38:08 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4B70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:08 [elasticsearch] DEBUG: > ["python工程师"]
2019-11-29 16:38:08 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:08 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (8): localhost:9200
2019-11-29 16:38:10 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:10 [elasticsearch] DEBUG: > ["python工程师"]
2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['北京以萨技术股份有限公司'],
 'company_url': ['http://company.zhaopin.com/CZ340780830.htm'],
 'degree_need': ['3-5年'],
 'job_advantage': ['五险一金,绩效奖金,通讯补助,带薪年假,节日福利'],
 'job_city': ['南京'],
 'job_need': ['<div '
              'class="describtion__detail-content"><div>岗位职责：</div><div>1、负责机房运维工具的设计与开发；</div><div>2、负责Devops运维管理平台的搭建与开发</div><div>3、数据库维护及优化；</div><div>4、系统部署、监控等；</div><div>5、大区内部python技术支持。</div><div>任职要求：</div><div>1.本科及以上学历；</div><div>2.熟练使用Git,熟练使用GO语言或Python</div><div>3.有前端开发知识储备，熟悉Javascript、CSS，html;</div><div>4.具备良好的编码习惯，缜密的接口设计能力；</div><div>5.理解Linux操作系统、体系结构，熟悉CentOS、ubuntu、RedHat系统,计算机网络，TCP/IP以及常用的网络协议；</div><div>6.熟悉常用的监控工具；</div><div>7.熟悉常用数据库（例如MySQL、Redis、Mongodb等）；</div><div>8.熟悉K8S/Istio，了解Envoy原理；</div><div>8.实践过持续集成与持续交付；</div><div>9.认同DevOps文化，有DevOps相关的经验;</div><div>10.优秀的学习能力，对新技术有高度热情，善于专研。</div></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月25日'],
 'salary': ['8千-1万'],
 'title': ['python工程师'],
 'work_years': ['雨花台区']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC000023354J00190524013.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CCL1218825980J00459728307.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC864094980J00435125803.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC862648410J00371601502.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC872220650J00199877409.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC896620630J00375792208.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CCL1213923340J00207119009.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC599476231J00380747308.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC599476231J00380747408.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CCL1203372320J00362190302.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC623461580J00407689207.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:10 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:10 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (9): localhost:9200
2019-11-29 16:38:12 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.040s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:12 [elasticsearch] DEBUG: > ["python开发工程师"]
2019-11-29 16:38:12 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:12 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (10): localhost:9200
2019-11-29 16:38:14 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BD9E8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:14 [elasticsearch] DEBUG: > ["python开发工程师"]
2019-11-29 16:38:14 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:14 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (11): localhost:9200
2019-11-29 16:38:16 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BD208>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:16 [elasticsearch] DEBUG: > ["python开发工程师"]
2019-11-29 16:38:16 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:16 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (12): localhost:9200
2019-11-29 16:38:18 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BDB70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:18 [elasticsearch] DEBUG: > ["python开发工程师"]
2019-11-29 16:38:18 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['南京锐意企创网络科技有限公司'],
 'company_url': ['http://company.zhaopin.com/CZ383859710.htm'],
 'degree_need': ['本科'],
 'job_advantage': ['年底双薪,五险一金,不加班,节日福利,员工旅游,周末双休,每年多次调薪'],
 'job_city': ['南京'],
 'job_need': ['<div '
              'class="describtion__detail-content"><div><h3>岗位职责</h3><ul><li>互联网数据爬取、处理和分析</li><li>设计爬虫策略和防屏蔽规则，提升网页抓取的效率和质量</li><li>对指定电商或金融类网站、移动端App进行数据抓取的开发工作</li><li>及时解决爬虫抓取过程中出现的问题并不断维护、优化程序</li></ul><h3>岗位要求</h3><ul><li>编程基础扎实， '
              '熟练使用Python语言， '
              '具备一定前端开发能力者优先</li><li>熟悉分布式,多线程,异步及高性能设计开发及优化</li><li>熟练使用MySQL等数据库</li><li>熟悉Linux操作系统</li><li>对图像识别、OCR技术有基本了解</li><li>有耐心、韧性、活力解决工作中的疑难问题，有自我要求，能替我提升</li><li>熟悉大数据相关技术(如HDFS/Elasticsearch/HBase/Redis/Spark等)优先</li></ul></div></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月14日'],
 'salary': ['8千-1万'],
 'title': ['python开发工程师'],
 'work_years': ['1-3年']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BDB70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5BDB70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5BDB70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC813110400J00461066101.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CZ829190630J00134964912.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC260972586J00326721903.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC645051428J00476628005.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC470601732J00357901506.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:18 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC692695929J00472420401.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:18 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:18 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (13): localhost:9200
2019-11-29 16:38:20 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4DA0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:20 [elasticsearch] DEBUG: > ["初级Python工程师助理（五险双休）"]
2019-11-29 16:38:20 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:20 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (14): localhost:9200
2019-11-29 16:38:22 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4C18>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:22 [elasticsearch] DEBUG: > ["初级Python工程师助理（五险双休）"]
2019-11-29 16:38:22 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:22 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (15): localhost:9200
2019-11-29 16:38:24 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4CF8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:24 [elasticsearch] DEBUG: > ["初级Python工程师助理（五险双休）"]
2019-11-29 16:38:24 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:24 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (16): localhost:9200
2019-11-29 16:38:26 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BDDD8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:26 [elasticsearch] DEBUG: > ["初级Python工程师助理（五险双休）"]
2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['南京正诺软件技术有限公司'],
 'company_url': ['http://company.zhaopin.com/CC628583138.htm'],
 'degree_need': ['大专'],
 'job_advantage': ['五险一金,绩效奖金,每年多次调薪,年底双薪,周末双休,不加班,员工旅游,交通补助'],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><p style="box-sizing: '
              'border-box; font-family: 微软雅黑; margin-bottom: 0px; border: 0px; '
              'color: rgb(51, 51, 51);">岗位描述：<br style="box-sizing: '
              'border-box;"></p><p style="box-sizing: border-box; font-family: '
              '微软雅黑; margin-bottom: 0px; border: 0px; color: rgb(51, 51, '
              '51);">1．参与公司核心系统项目的开发工作；</p><p style="box-sizing: border-box; '
              'font-family: 微软雅黑; margin-bottom: 0px; border: 0px; color: '
              'rgb(51, 51, 51);">2．能够根据项目的实际需要，进行技术方案的分析、选型；</p><p '
              'style="box-sizing: border-box; font-family: 微软雅黑; '
              'margin-bottom: 0px; border: 0px; color: rgb(51, 51, '
              '51);">3．保证系统稳健，代码质量，持续提供改进性建议并且实施</p><p style="box-sizing: '
              'border-box; font-family: 微软雅黑; margin-bottom: 0px; border: 0px; '
              'color: rgb(51, 51, 51);">\xa0</p><p style="box-sizing: '
              'border-box; font-family: 微软雅黑; margin-bottom: 0px; border: 0px; '
              'color: rgb(51, 51, 51);">任职要求：</p><p style="box-sizing: '
              'border-box; font-family: 微软雅黑; margin-bottom: 0px; border: 0px; '
              'color: rgb(51, 51, '
              '51);">1.大专及以上学历，本科理科生优先，提供实习岗，由架构师级别亲自带</p><p '
              'style="box-sizing: border-box; font-family: 微软雅黑; '
              'margin-bottom: 0px; border: 0px; color: rgb(51, 51, '
              '51);">2.熟悉软件开发语言，有相关工作经验优先</p><p style="box-sizing: border-box; '
              'font-family: 微软雅黑; margin-bottom: 0px; border: 0px; color: '
              'rgb(51, 51, 51);">3. 具有一定开发语言基础，具有较强的学习能力，积极努力，责任心强</p><p '
              'style="box-sizing: border-box; font-family: 微软雅黑; '
              'margin-bottom: 0px; border: 0px; color: rgb(51, 51, 51);">4. '
              '具有良好的沟通能力及团队协作意识</p><p style="box-sizing: border-box; '
              'font-family: 微软雅黑; margin-bottom: 0px; border: 0px; color: '
              'rgb(51, 51, 51);">5. 从事项目，是人工智能及大数据方向，未来前景大好！\xa0</p><p '
              'style="box-sizing: border-box; font-family: 微软雅黑; '
              'margin-bottom: 0px; border: 0px; color: rgb(51, 51, 51);">\xa0'
              '</p><p style="box-sizing: border-box; font-family: 微软雅黑; '
              'margin-bottom: 0px; border: 0px; color: rgb(51, 51, '
              '51);">薪资待遇：</p><p style="box-sizing: border-box; font-family: '
              '微软雅黑; margin-bottom: 0px; border: 0px; color: rgb(51, 51, '
              '51);">1周末双休，节假日休息</p><p style="box-sizing: border-box; '
              'font-family: 微软雅黑; margin-bottom: 0px; border: 0px; color: '
              'rgb(51, 51, 51);">2工作满一年可免费进行员工体检以及5天的带薪年假</p><span '
              'style="color: rgb(51, 51, 51); font-family: 微软雅黑; font-style: '
              'normal; font-weight: 400;">3公司可提供住宿，园区内有班车接送</span></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月15日'],
 'salary': ['2千-4千'],
 'title': ['初级Python工程师助理（五险双休）'],
 'work_years': ['经验不限']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BDDD8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5BDDD8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5BDDD8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC120107107J00197429814.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC120107107J00183057614.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC508620126J00335679405.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC569576922J00429457005.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC631016382J00443746507.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC143712520J00436820305.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC831062620J00390293707.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC645051428J00386044905.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC120107107J00183057514.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC196911813J00137056915.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC120107107J00076770814.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC678545520J00160239312.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC545530129J00246780105.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC120107107J00197429714.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC142715590J00367419908.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC678545520J00190671112.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:38:26 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:26 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (17): localhost:9200
2019-11-29 16:38:28 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BDEB8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:28 [elasticsearch] DEBUG: > ["Python程序员"]
2019-11-29 16:38:28 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:28 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (18): localhost:9200
2019-11-29 16:38:30 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:30 [elasticsearch] DEBUG: > ["Python程序员"]
2019-11-29 16:38:30 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:30 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (19): localhost:9200
2019-11-29 16:38:32 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4BE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:32 [elasticsearch] DEBUG: > ["Python程序员"]
2019-11-29 16:38:32 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:32 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (20): localhost:9200
2019-11-29 16:38:34 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4DA0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:34 [elasticsearch] DEBUG: > ["Python程序员"]
2019-11-29 16:38:34 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['江苏金恒信息科技股份有限公司'],
 'company_url': ['http://company.zhaopin.com/CZ201489330.htm'],
 'degree_need': ['本科'],
 'job_advantage': ['14薪,周末双休,五险一金,绩效奖金,加班补助,带薪年假,交通补助,餐补'],
 'job_city': ['南京'],
 'job_need': ['<div '
              'class="describtion__detail-content"><div>岗位描述：</div><div>1.负责搭建机器学习算法开发平台和创新技术引用；<br>2.能够配合算法工程师进行模型的封装和部署；<br>3.参与机器学习平台的架构设计和平台功能开发，功能迭代；</div><div>4.负责python程序的优化，多线程编程等任务<br></div><div>任职要求：</div><div>1. '
              '本科以上学历，有良好的计算机专业知识和扎实的编程功底；<br>2. 具有Web开发的相关项目经验，熟练掌握至少一门Python '
              'Web开发框架（Tornado、Django、Flask等)；<br>3. '
              '精通python语言，熟悉C#或者java语言，能够利用C#或者java进行简单的编程移植任务。<br>4. '
              '有独立分析和解决问题的能力，有良好的团队合作精神<br></div><div><br></div></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月28日'],
 'salary': ['6千-8千'],
 'title': ['Python程序员'],
 'work_years': ['1-3年']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4DA0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C4DA0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C4DA0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:34 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:34 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (21): localhost:9200
2019-11-29 16:38:36 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4EB8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:36 [elasticsearch] DEBUG: > ["Python数据分析实习生（量化金融方向）"]
2019-11-29 16:38:36 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:36 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (22): localhost:9200
2019-11-29 16:38:39 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8240>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:39 [elasticsearch] DEBUG: > ["Python数据分析实习生（量化金融方向）"]
2019-11-29 16:38:39 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:39 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (23): localhost:9200
2019-11-29 16:38:41 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C80B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:41 [elasticsearch] DEBUG: > ["Python数据分析实习生（量化金融方向）"]
2019-11-29 16:38:41 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:41 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (24): localhost:9200
2019-11-29 16:38:43 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8438>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:43 [elasticsearch] DEBUG: > ["Python数据分析实习生（量化金融方向）"]
2019-11-29 16:38:43 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['北京指南者前程教育科技有限公司'],
 'company_url': ['http://special.zhaopin.com/pagepublish/63269083/index.html'],
 'degree_need': ['硕士'],
 'job_advantage': ['周末双休,节日福利,14薪,住房补贴,五险一金,年底双薪,带薪年假,餐补'],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><p>一、公司介绍： '
              '</p><p>指南者教育成立于2012年，是一家提供留学申请服务、英语培训、数据分析培训的在线教育公司。我们相信，互联网对教育行业的改造将会是彻底的。秉承着“互联网+教育”的运营思路，指南者教育致力于打造让更多用户满意的学习及服务产品。</p><p>指南者教育的文化是“务实、分享、进取、正直”。务实：凡事要以结果为导向；分享：在帮助别人变得更好的同时，自己变得更好；进取：保持好奇心，探索自己未来的多种可能，正直：追随内心，恪守每个岗位的底线。</p><p>在这里你可以得到每年10天以上的年假，符合国家规定的五险一金以及带薪病假；</p><p>在这里没有办公室政治，鲜有上下级概念，团队氛围融洽；</p><p>在这里公司为每一位员工提供完善的培训体系，广阔的发展平台和晋升空间。</p><p><br></p><p>二、负责的工作：</p><p>1、参与量化金融方向PBL(Problem '
              'Based '
              'Learning)课程设计；</p><p>2、参与对学员教学成果的追踪、作业问题解答、实战项目指导，积极推动课程优化； '
              '</p><p>3、挖掘学员背景经历素材，为学员后续利用项目经历提供指导；</p><p>4、与其他部门紧密配合，完成其他配套材料（如资讯文章、公开课资料等）的准备。 '
              '</p><div>  '
              '<br></div><p>三、我们希望你：</p><p>1、商科方向在读研究生，有较强的定量分析能力；</p><p>2、能用文字、图形准确表达设计思路并展现内容，逻辑思维能力强；</p><p>3、具备良好的英语听说读写能力，口语能力优秀者优先考虑；</p><p>4、具有亲和力，优秀的沟通能力，乐于分享和传授经验。</p><p><br></p><p>四、你的福利： '
              '</p><p>1、有竞争力的实习薪酬（200/天），表现优异者奖金丰厚；</p><p>2、享受自由的发展空间，涉猎多个领域，在鼓励创新的氛围中快速成长；</p><p></p><p>3、咖啡奶茶零食肥宅快乐水免费提供。</p><p><br></p><p></p></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['09:49'],
 'salary': ['6千-8千'],
 'title': ['Python数据分析实习生（量化金融方向）'],
 'work_years': ['无经验']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8438>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8438>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8438>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:43 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:43 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (25): localhost:9200
2019-11-29 16:38:45 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.039s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4E10>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:45 [elasticsearch] DEBUG: > ["华为软件开发工程师c/c++/python_南京_北京_杭州_东莞"]
2019-11-29 16:38:45 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:45 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (26): localhost:9200
2019-11-29 16:38:47 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:47 [elasticsearch] DEBUG: > ["华为软件开发工程师c/c++/python_南京_北京_杭州_东莞"]
2019-11-29 16:38:47 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:47 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (27): localhost:9200
2019-11-29 16:38:49 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:49 [elasticsearch] DEBUG: > ["华为软件开发工程师c/c++/python_南京_北京_杭州_东莞"]
2019-11-29 16:38:49 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:49 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (28): localhost:9200
2019-11-29 16:38:51 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:51 [elasticsearch] DEBUG: > ["华为软件开发工程师c/c++/python_南京_北京_杭州_东莞"]
2019-11-29 16:38:51 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['华为技术有限公司'],
 'company_url': ['http://company.zhaopin.com/CC000542945.htm'],
 'degree_need': ['本科'],
 'job_advantage': ['绩效奖金,定期体检,五险一金'],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><div> '
              '一、专业知识：</div><div> '
              '1、熟悉掌握C/C++/Python编程语言之一，熟悉基本软件开发工程原理，掌握常用软件开发工具和调试手段；</div><div> '
              '2、具备良好的沟通协调能力和团队合作精神，热衷探索新技术，善于总结分享，喜欢动手实践。</div><div> '
              '二、业务技能：</div><div> '
              '1、承担软件模块设计开发，参与软件系统架构设计，参与软件系统技术竞争力构建，跟踪分析业界发展趋势，完成竞争力识别，标杆分析。</div><div> '
              '2、深入理解c/c++语言应用，具备大型软件产品设计和开发经验有软件重构和软件性能优化经验。</div><div> '
              '三、岗位职责</div><div> '
              '1、对标业界数据芯片业务SDK，基于自研数据芯片满足网络产品业务需求，提供统一的API服务，满足产品快速应用及切换芯片需求，支撑产品业务竞争力业界最佳；</div><div> '
              '2、负责VRP平台资源架构工具的部分设计、开发与测试工作；</div><div> '
              '3、负责同时支持多个产品的资源构建设计、构建效率优化和性能调优。</div><div> '
              '4、VRP平台资源白盒工具的设计、开发、测试</div></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['10月9日'],
 'salary': ['面议'],
 'title': ['华为软件开发工程师c/c++/python_南京_北京_杭州_东莞'],
 'work_years': ['1-3年']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C4D30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:51 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:51 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (29): localhost:9200
2019-11-29 16:38:53 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5BDEB8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:53 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:38:53 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:53 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (30): localhost:9200
2019-11-29 16:38:55 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:55 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:38:55 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:55 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (31): localhost:9200
2019-11-29 16:38:57 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A47B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:57 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:38:57 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:57 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (32): localhost:9200
2019-11-29 16:38:59 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4E80>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:38:59 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:38:59 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['南京擎盾信息科技有限公司'],
 'company_url': ['http://special.zhaopin.com/pagepublish/30593471/index.html'],
 'degree_need': ['学历不限'],
 'job_advantage': ['五险一金,绩效奖金,年终分红,加班补助,全勤奖,带薪年假,定期体检,节日福利'],
 'job_city': ['南京'],
 'job_need': ['<div '
              'class="describtion__detail-content"><p>岗位职责：<br></p><p>（1）负责Python '
              'Web后端项目开发；</p><p>（2）负责数据库模型设计；</p><p>（3）负责接口设计、开发；</p><p>（4）负责接口文档编写；</p><p>任职要求：</p><p>（1）计算机相关专业本科及以上学历，3年及以上Web后端开发经验</p><p>（2）熟悉Python和其Web框架（Django或Flask）</p><p>（3）会一种或多种数据库的操作，熟悉SQL</p><p>（4）熟悉Linux系统上的部署操作</p><p>（4）有Web+A1:D6</p><p>（5）有带领团队，项目负责经验优先</p><p>（6）有Redis, '
              'Celery, Docker使用经验者优先</p><p>（7）有Git使用经验者优先</p><p>（8）有Rest '
              'API设计经验的优先</p><p>（9）有敏捷开发经验或感兴趣的优先</p><p>（10）喜欢跟踪和尝试新的技术，追求编写优雅的代码，从技术趋势和思路上能影响技术团队</p></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月25日'],
 'salary': ['9千-1.8万'],
 'title': ['Python开发工程师'],
 'work_years': ['经验不限']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4E80>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A4E80>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A4E80>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:38:59 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:38:59 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (33): localhost:9200
2019-11-29 16:39:01 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:01 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:39:01 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:01 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (34): localhost:9200
2019-11-29 16:39:03 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8AC8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:03 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:39:03 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:03 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (35): localhost:9200
2019-11-29 16:39:05 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C85C0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:05 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:39:05 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:05 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (36): localhost:9200
2019-11-29 16:39:07 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:07 [elasticsearch] DEBUG: > ["Python开发工程师"]
2019-11-29 16:39:07 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['大汉软件股份有限公司'],
 'company_url': ['http://special.zhaopin.com/pagepublish/12092176/index.html'],
 'degree_need': ['1-3年'],
 'job_advantage': ['五险一金,带薪年假,餐补,绩效奖金,年底双薪,股票期权,定期体检,定期团建'],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content">岗位职责：<br><ol>  '
              '<li>负责机器学习、深度学习领域的研发及相关应用工作。</li>  '
              '<li>负责基于Python的数据分析、数据处理。</li>  '
              '<li>负责基于Python/Java机器学习模型封装、部署、上线。</li>  '
              '<li>利用各类有效的AI算法与公司各产品结合，完善和拓展产品功能。</li></ol><p>岗位要求</p><p></p><ol>  '
              '<li>本科及以上学历，至少1年Linux python使用经验；熟练使用Python，有良好的代码习惯。</li>  '
              '<li>至少熟悉Python主流框架Django、Tornado、Flask中的一种。</li>  '
              '<li>熟悉机器学习经典分类、聚类、回归、降维等算法与模型优先。</li>  <li>有Java开发经验优先。</li>  '
              '<li>熟练使用git, 有良好的编码习惯和风格。</li></ol></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月19日'],
 'salary': ['6千-1.2万'],
 'title': ['Python开发工程师'],
 'work_years': ['玄武区']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:39:07 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:07 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (37): localhost:9200
2019-11-29 16:39:09 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8A90>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:09 [elasticsearch] DEBUG: > ["Python数据分析课程老师"]
2019-11-29 16:39:09 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:09 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (38): localhost:9200
2019-11-29 16:39:11 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8390>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:11 [elasticsearch] DEBUG: > ["Python数据分析课程老师"]
2019-11-29 16:39:11 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:11 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (39): localhost:9200
2019-11-29 16:39:13 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8518>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:13 [elasticsearch] DEBUG: > ["Python数据分析课程老师"]
2019-11-29 16:39:13 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:13 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (40): localhost:9200
2019-11-29 16:39:15 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.019s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:15 [elasticsearch] DEBUG: > ["Python数据分析课程老师"]
2019-11-29 16:39:15 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['北京指南者前程教育科技有限公司'],
 'company_url': ['http://special.zhaopin.com/pagepublish/63269083/index.html'],
 'degree_need': ['硕士'],
 'job_advantage': ['14薪,周末双休,五险一金,绩效奖金,员工旅游,节日福利,免费培训'],
 'job_city': ['南京'],
 'job_need': ['<div '
              'class="describtion__detail-content"><p>一、公司介绍：</p><p>指南者教育成立于2012年，是一家提供留学申请服务、英语培训、数据分析培训的在线教育公司。我们相信，互联网对教育行业的改造将会是彻底的。秉承着“互联网+教育”的运营思路，指南者教育致力于打造让更多用户满意的学习及服务产品。</p><p>指南者教育的文化是“务实、分享、进取、正直”。务实：凡事要以结果为导向；分享：在帮助别人变得更好的同时，自己变得更好；进取：保持好奇心，探索自己未来的多种可能，正直：追随内心，恪守每个岗位的底线。</p><p>在这里你可以得到每年10天以上的年假，符合国家规定的五险一金以及带薪病假；</p><p>在这里没有办公室政治，鲜有上下级概念，团队氛围融洽；</p><p>在这里公司为每一位员工提供完善的培训体系，广阔的发展平台和晋升空间。</p><p><br></p><p> '
              '</p><p>二、具体的工作：</p><p>1.参与PBL(Problem-BasedLearning)教学方案设计，以数据分析项目为主轴，线上教授学生相关知识点，及指导学生进行项目操作；</p><p>2.进行数据分析项目的统筹规划，准备金融/市场营销/商业分析/传媒/教育等方向的数据分析项目，完成数据准备.场景设计.参考解答准备等环节；</p><p>3.负责对学员教学成果的追踪.学生作业问题解答，积极推动课程质量提升； '
              '</p><p>4.配合完成公司其他工作，如示范项目报告准备等；</p><p><br></p><p>三、我们希望你：</p><p>1.研究生学历，数学.统计.计算机.金融工程等专业背景，Python熟练使用者优先考虑；</p><p>2.具有快速学习能力，较强的逻辑分析能力，资料搜索能力和文字功底； '
              '</p><p>3.你可以没有经验没有资源，但要能够在工作中挖开你的脑洞，分享独特的思路；</p><p><br></p><p>四、你的福利：</p><p>1.享受同行业中有竞争力的薪酬，灵活的调薪机制（能者多劳，多劳多得）；</p><p>2.定期专业分享会与深度讲座，共同学习获得成长（斯坦福大神经验分享）；</p><p>3.宽松和谐的工作氛围，丰富免费的咖啡奶茶零食（提供肥宅快乐水供应）；</p><p>4.国内外旅游，部门聚餐和团建活动，生日小福利（开心生活，快乐工作）。</p><p></p><p>工作地点：南京/常州</p></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['09:49'],
 'salary': ['1万-1.5万'],
 'title': ['Python数据分析课程老师'],
 'work_years': ['经验不限']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:39:15 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:15 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (41): localhost:9200
2019-11-29 16:39:17 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8390>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:17 [elasticsearch] DEBUG: > ["Python爬虫实习生"]
2019-11-29 16:39:17 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:17 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (42): localhost:9200
2019-11-29 16:39:19 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8160>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:19 [elasticsearch] DEBUG: > ["Python爬虫实习生"]
2019-11-29 16:39:19 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:19 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (43): localhost:9200
2019-11-29 16:39:21 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8320>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:21 [elasticsearch] DEBUG: > ["Python爬虫实习生"]
2019-11-29 16:39:21 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:21 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (44): localhost:9200
2019-11-29 16:39:23 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:23 [elasticsearch] DEBUG: > ["Python爬虫实习生"]
2019-11-29 16:39:23 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['北京指南者前程教育科技有限公司'],
 'company_url': ['http://special.zhaopin.com/pagepublish/63269083/index.html'],
 'degree_need': ['硕士'],
 'job_advantage': ['周末双休,节日福利,餐补,免费培训'],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><div> '
              '一、公司介绍：</div><div> '
              '指南者教育成立于2012年，是一家提供留学申请服务、英语培训、数据分析培训的在线教育公司。我们相信，互联网对教育行业的改造将会是彻底的。秉承着“互联网+教育”的运营思路，指南者教育致力于打造让更多用户满意的学习及服务产品。</div><div> '
              '指南者教育的文化是“务实、分享、进取、正直”。务实：凡事要以结果为导向；分享：在帮助别人变得更好的同时，自己变得更好；进取：保持好奇心，探索自己未来的多种可能，正直：追随内心，恪守每个岗位的底线。</div><div> '
              '在这里你可以得到每年10天以上的年假，符合国家规定的五险一金以及带薪病假；</div><div> '
              '在这里没有办公室政治，鲜有上下级概念，团队氛围融洽；</div><div> '
              '在这里公司为每一位员工提供完善的培训体系，广阔的发展平台和晋升空间。</div><div> <br></div><div> '
              '二、岗位职责：</div><div> '
              '1、协助进行各类爬虫项目，如淘宝，知乎，微博，美团，大众点评等网站数据爬取；</div><div> '
              '2、参与爬虫课程和项目设计，协助完成课程准备、场景设计、练习设置等环节；</div><div> <br></div><div> '
              '三、岗位要求：</div><div> '
              '1、全日制在读研究生，掌握Python等至少一门语言，会使用常见软件进行抓包（fiddler或Charles），至少会使用浏览器抓包；</div><div> '
              '2、可以应对部分反爬技术（如模拟登录，验证码，IP问题，动态加载，重定向，数字加密等），熟悉正则，XPath抽取数据，有APP爬取数据的经验，会使用Scrapy框架优先；</div><div> '
              '3、具有快速学习能力，具有团队协作能力</div><div> <br></div><div> '
              '四、你的福利：</div><div> 1、有竞争力的实习薪酬（200/天），表现优异者奖金丰厚；</div><div> '
              '2、享受自由的发展空间，涉猎多个领域，在鼓励创新的氛围中快速成长；</div><div> '
              '3、咖啡奶茶零食肥宅快乐水免费提供。</div></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['09:49'],
 'salary': ['2千-4千'],
 'title': ['Python爬虫实习生'],
 'work_years': ['无经验']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C8C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:39:23 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:23 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (45): localhost:9200
2019-11-29 16:39:25 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C8908>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:25 [elasticsearch] DEBUG: > ["Python数据分析（量化金融）课程老师"]
2019-11-29 16:39:25 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:25 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (46): localhost:9200
2019-11-29 16:39:27 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4CF8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:27 [elasticsearch] DEBUG: > ["Python数据分析（量化金融）课程老师"]
2019-11-29 16:39:27 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:27 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (47): localhost:9200
2019-11-29 16:39:29 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4860>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:29 [elasticsearch] DEBUG: > ["Python数据分析（量化金融）课程老师"]
2019-11-29 16:39:29 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:29 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (48): localhost:9200
2019-11-29 16:39:31 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A47B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:31 [elasticsearch] DEBUG: > ["Python数据分析（量化金融）课程老师"]
2019-11-29 16:39:31 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['北京指南者前程教育科技有限公司'],
 'company_url': ['http://special.zhaopin.com/pagepublish/63269083/index.html'],
 'degree_need': ['硕士'],
 'job_advantage': ['14薪,周末双休,五险一金,绩效奖金,员工旅游,节日福利,免费培训'],
 'job_city': ['南京'],
 'job_need': ['<div '
              'class="describtion__detail-content"><p>一、公司介绍：</p><p>指南者教育成立于2012年，是一家提供留学申请服务、英语培训、数据分析培训的在线教育公司。我们相信，互联网对教育行业的改造将会是彻底的。秉承着“互联网+教育”的运营思路，指南者教育致力于打造让更多用户满意的学习及服务产品。</p><p>指南者教育的文化是“务实、分享、进取、正直”。务实：凡事要以结果为导向；分享：在帮助别人变得更好的同时，自己变得更好；进取：保持好奇心，探索自己未来的多种可能，正直：追随内心，恪守每个岗位的底线。</p><p>在这里你可以得到每年10天以上的年假，符合国家规定的五险一金以及带薪病假；</p><p>在这里没有办公室政治，鲜有上下级概念，团队氛围融洽；</p><p>在这里公司为每一位员工提供完善的培训体系，广阔的发展平台和晋升空间。</p><p><br></p><p> '
              '</p><p>二、具体的工作：</p><p>1、参与量化金融方向PBL(Problem Based '
              'Learning)课程设计，以数据分析为主轴，进行课程思路和体系设置，编写课程大纲及相关教学资料，完成配套训练设计；<br>2、负责对学员教学成果的追踪、作业问题解答、实战项目指导，积极推动课程优化； '
              '<br>3、挖掘学员背景经历素材，为学员后续利用项目经历提供指导；<br>4、与其他部门紧密配合，完成其他配套材料（如资讯文章、公开课资料等）的准备。 '
              '<br></p><p><br></p><p>三、我们希望你：</p><p>1、商科方向研究生毕业，有较强的定量分析能力；</p><p>2、能用文字、图形准确表达设计思路并展现内容，逻辑思维能力强；</p><p>3、具备良好的英语听说读写能力，口语能力优秀者优先考虑；</p><p>4、具有亲和力，优秀的沟通能力，乐于分享和传授经验。</p><p><br></p><p>四、你的福利：</p><p>1.享受同行业中有竞争力的薪酬，灵活的调薪机制（能者多劳，多劳多得）；</p><p>2.定期专业分享会与深度讲座，共同学习获得成长（斯坦福大神经验分享）；</p><p>3.宽松和谐的工作氛围，丰富免费的咖啡奶茶零食（提供肥宅快乐水供应）；</p><p>4.国内外旅游，部门聚餐和团建活动，生日小福利（开心生活，快乐工作）。</p><p></p><p>工作地点：南京/常州</p></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['09:49'],
 'salary': ['1万-1.5万'],
 'title': ['Python数据分析（量化金融）课程老师'],
 'work_years': ['经验不限']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A47B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A47B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A47B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:39:31 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:31 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (49): localhost:9200
2019-11-29 16:39:33 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4B70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:33 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师-初级"]
2019-11-29 16:39:33 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:33 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (50): localhost:9200
2019-11-29 16:39:36 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4C88>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:36 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师-初级"]
2019-11-29 16:39:36 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:36 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (51): localhost:9200
2019-11-29 16:39:38 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.031s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4128>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:38 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师-初级"]
2019-11-29 16:39:38 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:38 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (52): localhost:9200
2019-11-29 16:39:40 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4400>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:40 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师-初级"]
2019-11-29 16:39:40 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['Uniqsys优尼卡日本株式会社'],
 'company_url': ['http://company.zhaopin.com/CZ842910190.htm'],
 'degree_need': ['本科'],
 'job_advantage': [''],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><p '
              'class="MsoNormal"><b><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-weight:bold;font-size:12.0000pt;mso-font-kerning:1.0000pt;">※本职位为日本工作职位，简历请写明日语等级，不会日语的请勿投递，感谢配合！！！</span></b><b><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-weight:bold;font-size:12.0000pt;mso-font-kerning:1.0000pt;"><p></p></span></b></p><p '
              'class="MsoNormal"><b><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-weight:bold;font-size:12.0000pt;mso-font-kerning:1.0000pt;"><p>\xa0'
              '</p></span></b></p><p class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">工作地点：东京及周边</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p>\xa0'
              '</p></span></p><p class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">岗位要求：</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">1<font '
              'face="宋体">、计算机相关专业本科以上学历，日语</font><font '
              'face="Calibri">3</font><font '
              'face="宋体">级以上，能正确理解设计书内容，有过</font><font '
              'face="Calibri">2</font><font '
              'face="宋体">年以上对日开发经验或有日语能力</font><font '
              'face="Calibri">2</font><font '
              'face="宋体">级以上资格证书者优先录用；</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">2<font '
              'face="宋体">、有以下任何一门开发语言有</font><font '
              'face="Calibri">2</font><font '
              'face="宋体">年以上开发经验、或计算机专业毕业、或日语一计算机专业毕业即可：</font><font '
              'face="Calibri">JAVA,.NET,C#,C++/C,SAP,PHP,COBOL,IOS,Android,Salesforce,Python,CRM(dynamicsAX)</font><font '
              'face="宋体">等；有上述开发语言资格证书、</font><font '
              'face="Calibri">oracle</font><font face="宋体">金银牌、</font><font '
              'face="Calibri">CCNA</font><font face="宋体">或</font><font '
              'face="Calibri">CCNP</font><font '
              'face="宋体">等计算机软硬件相关证书者可优先录用；</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">3<font '
              'face="宋体">、要求身体健康，有敬业精神，对工作认真负责，能主动积极应对各项任务，听从指挥，服从安排，诚实守信。</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">4<font '
              'face="宋体">、能够接受长期日本出差。</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p>\xa0'
              '</p></span></p><p class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">薪资待遇：</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">日本员工同等待遇，视日语水平，工作经验而定（</font>20<font '
              'face="宋体">万～</font><font face="Calibri">60</font><font '
              'face="宋体">万日元</font><font face="Calibri">/</font><font '
              'face="宋体">月）</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">免费宿舍，交通补助，定期加薪，免费培训，定期团建，员工旅游，节日福利等</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月28日'],
 'salary': ['5千-1万'],
 'title': ['日语Java/Python开发工程师-初级'],
 'work_years': ['经验不限']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4400>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C4400>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5C4400>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:39:40 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:40 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (53): localhost:9200
2019-11-29 16:39:42 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5C4E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:42 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师"]
2019-11-29 16:39:42 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:42 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (54): localhost:9200
2019-11-29 16:39:44 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5D7860>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:44 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师"]
2019-11-29 16:39:44 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:44 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (55): localhost:9200
2019-11-29 16:39:46 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.047s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5D77F0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:46 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师"]
2019-11-29 16:39:46 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:46 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (56): localhost:9200
2019-11-29 16:39:48 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.017s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5D79B0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:48 [elasticsearch] DEBUG: > ["日语Java/Python开发工程师"]
2019-11-29 16:39:48 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['Uniqsys优尼卡日本株式会社'],
 'company_url': ['http://company.zhaopin.com/CZ842910190.htm'],
 'degree_need': ['本科'],
 'job_advantage': ['加班补助,包住,交通补助,房补,住房补贴,无试用期,周末双休,定期团建'],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><p '
              'class="MsoNormal"><b><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-weight:bold;font-size:12.0000pt;mso-font-kerning:1.0000pt;">※本职位为日本工作职位，简历请写明日语等级，不会日语的请勿投递，感谢配合！！！</span></b><b><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-weight:bold;font-size:12.0000pt;mso-font-kerning:1.0000pt;"><p></p></span></b></p><p '
              'class="MsoNormal"><b><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-weight:bold;font-size:12.0000pt;mso-font-kerning:1.0000pt;"><p>\xa0'
              '</p></span></b></p><p class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">工作地点：东京及周边</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p>\xa0'
              '</p></span></p><p class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">岗位要求：</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">1<font '
              'face="宋体">、计算机相关专业本科以上学历，日语</font><font '
              'face="Calibri">3</font><font '
              'face="宋体">级以上，能正确理解设计书内容，有过</font><font '
              'face="Calibri">2</font><font '
              'face="宋体">年以上对日开发经验或有日语能力</font><font '
              'face="Calibri">2</font><font '
              'face="宋体">级以上资格证书者优先录用；</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">2<font '
              'face="宋体">、有以下任何一门开发语言有</font><font '
              'face="Calibri">2</font><font '
              'face="宋体">年以上开发经验、或计算机专业毕业、或日语一计算机专业毕业即可：</font><font '
              'face="Calibri">JAVA,.NET,C#,C++/C,SAP,PHP,COBOL,IOS,Android,Salesforce,Python,CRM(dynamicsAX)</font><font '
              'face="宋体">等；有上述开发语言资格证书、</font><font '
              'face="Calibri">oracle</font><font face="宋体">金银牌、</font><font '
              'face="Calibri">CCNA</font><font face="宋体">或</font><font '
              'face="Calibri">CCNP</font><font '
              'face="宋体">等计算机软硬件相关证书者可优先录用；</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">3<font '
              'face="宋体">、要求身体健康，有敬业精神，对工作认真负责，能主动积极应对各项任务，能听从指挥，不消极找借口等靠要，诚实守信，做事耐心。</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;">4<font '
              'face="宋体">、能够接受长期日本出差。</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p>\xa0'
              '</p></span></p><p class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">薪资待遇：</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">日本员工同等待遇，视日语水平，工作经验而定（</font>20<font '
              'face="宋体">万～</font><font face="Calibri">60</font><font '
              'face="宋体">万日元</font><font face="Calibri">/</font><font '
              'face="宋体">月）</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p><p '
              'class="MsoNormal"><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><font '
              'face="宋体">免费宿舍，交通补助，定期加薪，免费培训，定期团建，员工旅游，节日福利等</font></span><span '
              'style="mso-spacerun:\'yes\';font-family:宋体;mso-ascii-font-family:Calibri;mso-hansi-font-family:Calibri;mso-bidi-font-family:\'Times '
              'New '
              'Roman\';font-size:10.5000pt;mso-font-kerning:1.0000pt;"><p></p></span></p></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['11月28日'],
 'salary': ['1.5万-3万'],
 'title': ['日语Java/Python开发工程师'],
 'work_years': ['经验不限']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5D79B0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5D79B0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5D79B0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:39:48 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:48 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (57): localhost:9200
2019-11-29 16:39:50 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:50 [elasticsearch] DEBUG: > ["Python工程师"]
2019-11-29 16:39:50 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:50 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (58): localhost:9200
2019-11-29 16:39:52 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.000s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4E48>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:52 [elasticsearch] DEBUG: > ["Python工程师"]
2019-11-29 16:39:52 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:52 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (59): localhost:9200
2019-11-29 16:39:54 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4B70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:54 [elasticsearch] DEBUG: > ["Python工程师"]
2019-11-29 16:39:54 [urllib3.util.retry] DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0, status=None)
2019-11-29 16:39:54 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (60): localhost:9200
2019-11-29 16:39:56 [elasticsearch] WARNING: GET http://localhost:9200/lagou/_analyze?filter=%5B%27lowercase%27%5D&analyzer=ik_max_word [status:N/A request:2.016s]
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2019-11-29 16:39:56 [elasticsearch] DEBUG: > ["Python工程师"]
2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error processing {'company_name': ['上海露善文化传播有限公司'],
 'company_url': ['http://company.zhaopin.com/CZ872325800.htm'],
 'degree_need': ['经验不限'],
 'job_advantage': ['五险一金,年底双薪,绩效奖金,不加班,节日福利,项目奖金,定期团建,带薪年假'],
 'job_city': ['南京'],
 'job_need': ['<div class="describtion__detail-content"><p '
              "style='margin-bottom: 0px; color: rgb(51, 51, 51); font-family: "
              '"microsoft yahei";\'>岗位职责</p><p style=\'margin-bottom: 0px; '
              'color: rgb(51, 51, 51); font-family: "microsoft '
              'yahei";\'>1、参与需求分析，产品设计，功能开发；</p><p style=\'margin-bottom: 0px; '
              'color: rgb(51, 51, 51); font-family: "microsoft '
              'yahei";\'>2、负责网络攻防平台系统开发、日常维护；</p><p style=\'margin-bottom: '
              '0px; color: rgb(51, 51, 51); font-family: "microsoft '
              'yahei";\'>3、 生产环境部署、监控与优化。</p><p style=\'margin-bottom: 0px; '
              'color: rgb(51, 51, 51); font-family: "microsoft '
              'yahei";\'>4、与团队技术交流，共同进步。</p><p style=\'margin-bottom: 0px; '
              'color: rgb(51, 51, 51); font-family: "microsoft '
              'yahei";\'><br></p><p style=\'margin-bottom: 0px; color: rgb(51, '
              '51, 51); font-family: "microsoft yahei";\'>任职要求</p><p '
              "style='margin-bottom: 0px; color: rgb(51, 51, 51); font-family: "
              '"microsoft yahei";\'>1、计算机相关专业，全日制大学本科以上学历，接触过linux优先</p><p '
              "style='margin-bottom: 0px; color: rgb(51, 51, 51); font-family: "
              '"microsoft yahei";\'>2、接触过linux优先</p><p style=\'margin-bottom: '
              '0px; color: rgb(51, 51, 51); font-family: "microsoft '
              'yahei";\'>3、具备良好的编码习惯及开发文档书写习惯；</p></div>'],
 'job_responsibility': [''],
 'job_type': ['全职'],
 'job_url': ['来自智联招聘'],
 'publish_time': ['10:41'],
 'salary': ['8千-1万'],
 'title': ['Python工程师'],
 'work_years': ['建邺区']}
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 159, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw)
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 80, in create_connection
    raise err
  File "g:\anaconda\lib\site-packages\urllib3\util\connection.py", line 70, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 115, in perform_request
    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 638, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "g:\anaconda\lib\site-packages\urllib3\util\retry.py", line 343, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "g:\anaconda\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 600, in urlopen
    chunked=chunked)
  File "g:\anaconda\lib\site-packages\urllib3\connectionpool.py", line 354, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "g:\anaconda\lib\http\client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "g:\anaconda\lib\http\client.py", line 1026, in _send_output
    self.send(msg)
  File "g:\anaconda\lib\http\client.py", line 964, in send
    self.connect()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 181, in connect
    conn = self._new_conn()
  File "g:\anaconda\lib\site-packages\urllib3\connection.py", line 168, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000003B4C5A4828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\pipelines.py", line 12, in process_item
    item.save_to_es()
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 57, in save_to_es
    job.suggest = gen_suggests(JobType._doc_type.index, ((job.title, 10), (job.job_city, 7)))
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\items.py", line 73, in gen_suggests
    words = es.indices.analyze(index=index, analyzer="ik_max_word", params={'filter': ["lowercase"]}, body=text)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\utils.py", line 73, in _wrapped
    return func(*args, params=params, **kwargs)
  File "g:\anaconda\lib\site-packages\elasticsearch\client\indices.py", line 32, in analyze
    '_analyze'), params=params, body=body)
  File "g:\anaconda\lib\site-packages\elasticsearch\transport.py", line 312, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File "g:\anaconda\lib\site-packages\elasticsearch\connection\http_urllib3.py", line 124, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A4828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x0000003B4C5A4828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。)
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC447954822J00406617303.htm"}
2019-11-29 16:39:56 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/239912717250164.htm"}
2019-11-29 16:39:56 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CZ896695330J00246985206.htm"}
2019-11-29 16:39:56 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC218653388J00387165003.htm"}
2019-11-29 16:39:56 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://127.0.0.1:54302/session/fdb7d37a-28dd-4563-908c-4347aaedf287/url {"url": "https://jobs.zhaopin.com/CC824472920J00417432405.htm"}
2019-11-29 16:39:56 [urllib3.connectionpool] DEBUG: http://127.0.0.1:54302 "POST /session/fdb7d37a-28dd-4563-908c-4347aaedf287/url HTTP/1.1" 404 123
2019-11-29 16:39:56 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC640402330J00378495702.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC120914506J00446169003.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC826330310J00327011508.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC191215511J00310135408.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC833252140J00274344707.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC645926286J00260826405.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CZ185718010J00196024502.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC465341913J00108419313.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CZ594912420J00305115703.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://xiaoyuan.zhaopin.com/job/CC000117819J90001986000?from=sz>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CCL1222977140J00450113003.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC542445930J00376959002.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC528943025J00441993307.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC833252140J00289577007.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC219838732J00353648902.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC219838732J00353649002.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC250519519J00365838608.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CZ697762580J00254850503.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/239912717250164.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC447954822J00406617303.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CZ896695330J00246985206.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC824472920J00417432405.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.scraper] ERROR: Error downloading <GET https://jobs.zhaopin.com/CC218653388J00387165003.htm>
Traceback (most recent call last):
  File "g:\anaconda\lib\site-packages\twisted\internet\defer.py", line 1386, in _inlineCallbacks
    result = g.send(result)
  File "g:\anaconda\lib\site-packages\scrapy\core\downloader\middleware.py", line 37, in process_request
    response = yield method(request=request, spider=spider)
  File "C:\Users\asus\Desktop\爬虫\scrapy教程\zhisousou\zhisousou\middlewares.py", line 94, in process_request
    self.driver.get(request.url)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 333, in get
    self.execute(Command.GET, {'url': url})
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 321, in execute
    self.error_handler.check_response(response)
  File "g:\anaconda\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.InvalidSessionIdException: Message: Tried to run command without establishing a connection

2019-11-29 16:39:56 [scrapy.core.engine] INFO: Closing spider (finished)
2019-11-29 16:39:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 75,
 'downloader/exception_type_count/selenium.common.exceptions.InvalidSessionIdException': 74,
 'downloader/exception_type_count/selenium.common.exceptions.WebDriverException': 1,
 'downloader/response_bytes': 4560824,
 'downloader/response_count': 17,
 'downloader/response_status_count/200': 17,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 11, 29, 8, 39, 56, 614701),
 'log_count/DEBUG': 582,
 'log_count/ERROR': 90,
 'log_count/INFO': 12,
 'log_count/WARNING': 60,
 'request_depth_max': 2,
 'response_received_count': 17,
 'scheduler/dequeued': 92,
 'scheduler/dequeued/memory': 92,
 'scheduler/enqueued': 92,
 'scheduler/enqueued/memory': 92,
 'start_time': datetime.datetime(2019, 11, 29, 8, 36, 33, 205259)}
2019-11-29 16:39:56 [scrapy.core.engine] INFO: Spider closed (finished)
